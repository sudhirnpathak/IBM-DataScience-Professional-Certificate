{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression methods can be used to predict a continuous value (target) using some other variables (features). In regression, there are two types of variables, a dependent variable (target, typically denoted by Y) and one or more independent variables (features, typically denoted by X). Our regression models relates Y as a function of X. The key point in the regression is that our dependent variable Y should be continuous and can not be a discrete value. However, the independent variables can be measured on categorial or continous measurement scales.\n",
    "\n",
    "Basically, there are two types of regression models :\n",
    "<ul>\n",
    "<li>Simple Regression :: When only one independent variable is used to estimate the dependent variable. It can be either linear or non-linear regression, depending on the relationship between the independent and dependent variable.</li>\n",
    "    \n",
    "<li>Multiple Regression :: When more than one independent variables are used in modelling. Again, this regression can be linear or non-linear.</li>\n",
    "</ul>\n",
    "\n",
    "We have many regression algorithms, each of them has their own importance and specific conditions to which their application is best suited.\n",
    "<ul>\n",
    "<li> Ordinal regression </li>\n",
    "<li> Poisson regression </li>\n",
    "<li> Fast forest quantile regression </li>\n",
    "<li> Linear, Polynomial, Lasso, Stepwise, Ridge regression </li>\n",
    "<li> Bayesian Linear Regression </li>\n",
    "<li> Neural network regression </li>\n",
    "<li> Decision forest regression </li>\n",
    "<li> Boosted decision tree regression </li>\n",
    "<li> KNN (K-nearest neighbors) </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is the approximation of a linear model used to describe the relationship between two or more variables. Linear Regression fits a linear model $\\hat{y} = \\theta_0+\\theta_1x_1+\\theta_2x_2+...$ with coefficients (also called parameters) $\\theta = (\\theta_0,\\theta_1, ..., \\theta_n)$ to minimize the 'residual sum of squares. Here $x_i$'s are independent feature variables and $\\hat{y}$ is the prediction made by model given a certain values of $x_i$'s.\n",
    "\n",
    "The mean of all residual errors ($y-\\hat{y}$) is a measure of how the model fits with the dataset:\n",
    "\n",
    "\\begin{equation*}\n",
    "MSE = \\frac{1}{n}\\sum_{i}^{n}(y_i - \\hat{y}_i)^2\n",
    "\\end{equation*}\n",
    "\n",
    "where $y_i$, $\\hat{y}_i$ are the actual and predicted values respectively, and $n$ is the number of sample data points. Our objective is to find the parameters values $\\theta_i$'s such that MSE is minimized. \n",
    "\n",
    "We have two options to determine these error minimizing $\\theta_i$'s: \n",
    "<ui>\n",
    "<li> mathematical approach : for a simple linear regressin with only one independent variable\n",
    "    \\begin{equation*}\n",
    "    \\theta_1 = \\frac{\\sum_i^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_i^n(x_i-\\bar{x})^2}; \\theta_0 = \\bar{y} - \\theta_1\\bar{x}\n",
    "    \\end{equation*}\n",
    "    where $\\bar{x}, \\bar{y}$ represent mean values of $x_i$'s and $y_i$'s respectively.\n",
    "\n",
    "</li>\n",
    "<li> optimization approach : gradient descent (this approach is efficient for very large dataset, as the mathematical approach deals with large matrices and becomes inefficient)</li>\n",
    "</ui>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of regression is to build a model to accurately predict an unseen case. To this end, we have to perform evaluation of our model. When considering evalution of models, we clearly want to choose the one that gives the most accurate result. Thus, the question is how we can estimate the accuracy of our models. In other words, how much we can trust this model in prediction of an unknown sample using the given dataset and the model we have built.\n",
    "\n",
    "One of the solution is train-test split, in which we split the dataset into mutually exclusive training and test dataset. We use the training set for building the model. Now we can use the model to make prediction on the test set and see how the predicted values on the test set compare with the actual values of the test set. This indicates how accurate how model is. Once, we have selected the model with best accuracy, we train our model on the whole dataset, as we do not want to lose any valueable data for training.\n",
    "\n",
    "The issue with train-test is that it is highly dependent on the dataset on which the data was trained and tested. This can be resolved by using another evaluation model called K-fold cross validation. In K-fold cross validation, we split the whole data into K number of folds. We use K-1 folds for training and building the model and the remaining one is used for testing and evaluating the model. We continue this process until each fold is used for testing the model, finally averaging the correctness evalution of all the folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know that we have to compare the actual values and predicted values to estimate the accuracy of a regression model. For this purpose, we need accuracy metrics to quantify the accuracy of models. Evaluation metrics are used to explain the performance of a model. Some model evalution metrics that are used for regression are:\n",
    "<ul>\n",
    "    <li> Mean absolute error(MAE): It is the mean of the absolute value of the errors. This is the easiest of the metrics to understand since it’s just average error.\n",
    "        \\begin{equation*}\n",
    "        MAE = \\frac{1}{n}\\sum_{i=1}^n|y_i - \\hat{y}_i|\n",
    "        \\end{equation*}\n",
    "    </li>\n",
    "    <li>Mean Squared Error (MSE): Mean sqaured error is the mean of the squared error. \n",
    "        \\begin{equation*}\n",
    "        MSE = \\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat{y}_i)^2\n",
    "        \\end{equation*}\n",
    "        It’s more popular than Mean absolute error because the focus is geared more towards large errors. This is due to the squared term exponentially increasing larger errors in comparison to smaller ones.\n",
    "    </li>\n",
    "    <li>Root Mean Squared Error (RMSE): This is the square root of the Mean Squared Error.\n",
    "        \\begin{equation*}\n",
    "        RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}\n",
    "        \\end{equation*}\n",
    "        This is one of the most popular of the evaluation metrics because RMSE is in the same units as the target variable, making it easy to interpret.\n",
    "    </li>\n",
    "    <li>Relative Absolute Error (RAE):\n",
    "        \\begin{equation*}\n",
    "        RAE = \\frac{\\sum_{i=1}^n|y_i - \\hat{y}_i|}{\\sum_{i=1}^n|y_i - \\bar{y}|}\n",
    "        \\end{equation*}\n",
    "    </li>\n",
    "    <li>Relative Squared Error (RSE):\n",
    "        \\begin{equation*}\n",
    "        RSE = \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2}\n",
    "        \\end{equation*}\n",
    "        R-squared = 1-RSE, is not an error but is a popular metric for estimating the accuracy of model. It represents how close the data values are to the fitted regression line. The higher the R-squared, the better the model fits the data. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple linear regression, there are two variables, a dependent variable and an independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fuel consumption dataset contains model-specific fuel consumption ratings and estimated carbon dioxide emissions for new light-duty vehicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FuelConsumption.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()                        # to get basic statistics about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select some features for futher exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df[['ENGINESIZE', 'CYLINDERS', 'FUELCONSUMPTION_COMB', 'CO2EMISSIONS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot each of these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = cdf[['ENGINESIZE', 'CYLINDERS', 'FUELCONSUMPTION_COMB', 'CO2EMISSIONS']]\n",
    "viz.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot these features vs the Emission to see how linear is their relation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cdf.FUELCONSUMPTION_COMB, cdf.CO2EMISSIONS, color='blue')\n",
    "plt.xlabel('FUELCONSUMPTION_COMB')\n",
    "plt.ylabel('CO2EMISSIONS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS, color='blue')\n",
    "plt.xlabel('ENGINESIZE')\n",
    "plt.ylabel('CO2EMISSIONS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split our dataset into train and test sets, 80% of the entire data for training, and the rest 20% for testing. We create a mask to select random rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(cdf)) < 0.8\n",
    "train = cdf[msk]\n",
    "test = cdf[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use scikit-learn to model engine-size vs Co2 emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()\n",
    "train_x = np.asanyarray(train[['ENGINESIZE']])\n",
    "train_y = np.asanyarray(train[['CO2EMISSIONS']])\n",
    "lr.fit(train_x, train_y)\n",
    "print(lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data alongwith the fitted line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS, color='blue')\n",
    "plt.plot(train_x, lr.coef_[0][0]*train_x + lr.intercept_[0], '-r')\n",
    "plt.xlabel('ENGINESIZE')\n",
    "plt.ylabel('CO2EMISSIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "test_x = np.asanyarray(test[['ENGINESIZE']])\n",
    "test_y = np.asanyarray(test[['CO2EMISSIONS']])\n",
    "test_yhat = lr.predict(test_x)\n",
    "\n",
    "print('mean absolute error : %.2f' % np.mean(np.absolute(test_yhat - test_y)))\n",
    "print('residual sum of squares (MSE) : %.2f' % np.mean((test_yhat - test_y)**2))\n",
    "print('R2-score : %.2f' % r2_score(test_yhat, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

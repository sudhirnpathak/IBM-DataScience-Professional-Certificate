{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a supervised learning approach, to categorize unknown items into a discrete set of categories/classes. Classification attempts to learn the relationship between a set of feature variables and a target variable of interest. The target variable in classification is a categorial variable with discrete values.\n",
    "\n",
    "Classification algorithm in maching learning:\n",
    "\n",
    "- Decision Trees\n",
    "- Naive Bayes\n",
    "- Linear Discrimant Analysis\n",
    "- K-Nearest Neighbor\n",
    "- Logistic Regression\n",
    "- Neural Networks\n",
    "- Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors** is a classification algorithm, where it takes into account the K numbers of its nearest neighbors (data points) when it has to make a prediction (determine the class) of a new data point. In simple words, suppose I want to find the class of a new data point. Assume that we have chosen K = 10 to train our model. Then, I will look at the class labels of my 10 nearest neighbors and assign the majority label to my new data point.\n",
    "\n",
    "The K-Nearest Neighbors algorith is:\n",
    "\n",
    " - Choose a value of K\n",
    " - Calculate the distance of unknown case from all cases\n",
    " - Select the training observations in the training data that are nearest to the unknown data point\n",
    " - Predict the response of the unknown data point using the most popular response value from the K-nearest neighbors\n",
    "\n",
    "This algorithm is based on the assumption that data points closer (smaller distance) have same class. One measure of distance is Euclidean distance:\n",
    "\n",
    "$$\\text{distance between data point } i, j = \\sqrt{(x_{1,i} - x_{1,j})^2 + (x_{2,i} - x_{2,j})^2 + ...}$$\n",
    "\n",
    "where $x_1, x_2 ...$ are our feature variables.\n",
    "\n",
    "Choosing best K : Use train/test split. Train the model for various values of K, and pick the K for which the model performs best on the test set.\n",
    "\n",
    "K-Nearest Neighbor algorith can also be used to compute values for a continous target (regression), where we can use the mean or median value of the neighboring data points as the target value for the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will load a customer dataset related to a telecommunication company, use KNN to predict the category of customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a telecommunications provider has segmented its customer base by service usage patterns, categorizing the customers into four groups. If demographic data can be used to predict group membership, the company can customize offers for individual prospective customers. It is a classification problem. That is, given the dataset,  with predefined labels (the label for classes), we need to build a model to be used to predict class of a new/unknown case.\n",
    "\n",
    "The example focuses on using demographic data, such as region, age, and marital status, to predict usage patterns. The target field, called __custcat__, has four possible values that correspond to the four customer groups, as follows:\n",
    "<ui>\n",
    "<li> 1 : Basic Service </li>\n",
    "<li> 2 : E-Service </li>\n",
    "<li> 3 : Plus Service </li>\n",
    "<li> 4 : Total Service </li>\n",
    "</ui>\n",
    "\n",
    "Our objective is to build a classifier, to predict the class of unknown cases. We will use a specific type of classification called K-nearest neighbour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('teleCust1000t.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['custcat'].value_counts()              # to see how many of each class is in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our feature set X and label Y. To use scikit-learn library, we have to convert the Pandas data frame to a Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['region', 'tenure', 'age', 'marital', 'address', 'income', 'ed', 'employ', 'retire', 'gender', 'reside']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['custcat'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize our feature X. Data Standardization give data zero mean and unit variance. It is a good practice, especially for algorithms such as KNN which are based on distance between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use train-test-split to split the dataset into mutually exclusive training and testing sets. Then, we train the model with the training set and test with the testing set. Out-of-sample accuracy is the percentage of correct predictions that the model makes on the data that the model has not been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 4)\n",
    "\n",
    "print('Training dataset size : ', X_train.shape, Y_train.shape)\n",
    "print('Testing dataset size :', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification : KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose k = 4 (as we already know we have four classes), and train the model on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4                                                           \n",
    "neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train, Y_train)\n",
    "neigh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the trained model to prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred = neigh.predict(X_train)\n",
    "Y_test_pred = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multilabel classification, __accuracy classification score__ is a function that computes subset accuracy. This function is equal to the jaccard_similarity_score function. Essentially, it calculates how closely the actual labels and predicted labels are matched in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Train set accuracy : ', metrics.accuracy_score(Y_train, Y_train_pred))\n",
    "print('Test set accuracy : ', metrics.accuracy_score(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting K in KNN:** K in KNN, is the number of nearest neighbors to examine, which is supposed to be specified by the User. So, how can we choose right value for K ? The general solution is to reserve a part of the data for testing the accuracy of the model. Then chose k =1, use the training part for modeling, and calculate the accuracy of prediction using all samples in the test set. Repeat this process, increasing the k, and see which k is the best for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Krange = 10\n",
    "meanAccuracy = np.zeros((Krange-1))\n",
    "stdAccuracy = np.zeros((Krange-1))\n",
    "\n",
    "for n in range(1, Krange):\n",
    "    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train, Y_train)\n",
    "    Y_test_pred = neigh.predict(X_test)\n",
    "    meanAccuracy[n-1] = metrics.accuracy_score(Y_test, Y_test_pred)\n",
    "    stdAccuracy[n-1] = np.std(Y_test_pred == Y_test)/np.sqrt(Y_test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot model accuracy for different number of neighbors K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,Krange), meanAccuracy, 'g')\n",
    "plt.fill_between(range(1,Krange), meanAccuracy-1*stdAccuracy, meanAccuracy+1*stdAccuracy, alpha=0.1)\n",
    "plt.xlabel('Number of neighbors K')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(('Accuracy', '+/- 3xstd'))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The best accuracy obtained is', meanAccuracy.max(), 'with K = ', meanAccuracy.argmax()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
